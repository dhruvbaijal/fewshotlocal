{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/at677/fewshotlocal'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import NLLLoss\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "from helpful_files.networks import PROTO, avgpool, covapool, pL, pCL, fsL, fsCL, fbpredict\n",
    "from helpful_files.testing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Important Values\n",
    "\n",
    "# General settings\n",
    "datapath = '/data/dww78/mini_inat_shrunk/'   # The location of your train, test, repr, and query folders. Make sure it ends in '/'!\n",
    "model = 'myModel.pth'  # What model do you wish to evaluate, and where is it saved?\n",
    "gpu = 1                             # What gpu do you wish to run on?\n",
    "workers = 1                         # Number of cpu worker processes to use for data loading\n",
    "verbosity = 10                      # How many categories in between status updates \n",
    "ensemble = 4                        # How many models to evaluate in parallel\n",
    "k = 1                               # Evaluate top-k accuracy. Typically 1 or 5. \n",
    "torch.cuda.set_device(gpu) \n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Model characteristics\n",
    "covariance_pooling = False           # Did your model use covariance pooling?\n",
    "localizing = True                   # Did your model use localization?\n",
    "fewshot_local = True                # If you used localization: few-shot, or parametric? Few-shot if True, param if False\n",
    "network_width = 64                  # Number of channels at every layer of the network\n",
    "\n",
    "# Batch construction\n",
    "bsize = 64                          # Batch size\n",
    "boxes_available = 10                # Percentage of images with bounding boxes available (few-shot localization models only)\n",
    "include_masks = (localizing         # Include or ignore the bounding box annotations?\n",
    "                 and fewshot_local)\n",
    "n_trials = (10                      # Number of trials (few-shot localization models only)\n",
    "            if include_masks else 1)\n",
    "\n",
    "\n",
    "# Calculate embedding size based on model setup\n",
    "d = (network_width if not \n",
    "     covariance_pooling else\n",
    "     network_width**2)\n",
    "if localizing and not covariance_pooling:\n",
    "    d = network_width*2\n",
    "assert n_trials == 1 or include_masks, (\"Repeated trials will yield repeated identical results under this configuration.\"+\n",
    "                                        \"Please set ntrials to 1 or use a few-shot localizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load Testing Data\n",
    "\n",
    "d_boxes = torch.load('/data/db638/fewshotlocal/helpful_files/box_coords.pth')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4905, 0.4961, 0.4330],std=[0.1737, 0.1713, 0.1779])\n",
    "    ])\n",
    "\n",
    "repr_dataset = datasets.ImageFolder(\n",
    "    datapath+'repr', \n",
    "    loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "query_dataset = datasets.ImageFolder(\n",
    "    datapath+'query',\n",
    "    loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "repr_loader = torch.utils.data.DataLoader(\n",
    "    repr_dataset, \n",
    "    batch_sampler = OrderedSampler(repr_dataset, bsize),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "query_loader = torch.utils.data.DataLoader(\n",
    "    query_dataset,\n",
    "    batch_sampler = OrderedSampler(query_dataset, bsize),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "way = len(repr_dataset.classes)\n",
    "\n",
    "# Determine number of images with bounding boxes per-class\n",
    "catsizes = torch.LongTensor(np.array([t[1] for t in repr_dataset.imgs])).bincount().float()\n",
    "ngiv = (catsizes*boxes_available//100)\n",
    "for i in range(ngiv.size(0)):\n",
    "    if ngiv[i] == 0:\n",
    "        ngiv[i] = 1\n",
    "ngiv = ngiv.long().tolist()\n",
    "\n",
    "print('Data loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d_boxes = dict()\n",
    "for key, value in d_boxes.items():\n",
    "    new_key = \"/data/dww78/mini_inat_shrunk/\"+key[3:]\n",
    "    new_d_boxes[new_key] = value\n",
    "\n",
    "d_boxes = new_d_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/dww78/mini_inat_shrunk/train/3171/4c72b1a7b6a86b8de95425db8cd03384.bmp\n"
     ]
    }
   ],
   "source": [
    "print(list(d_boxes.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv3 = conv3x3(planes, planes)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "      \n",
    "        self.conv4 = conv3x3(inplanes, planes)\n",
    "        self.bn4 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(stride)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.num_batches_tracked = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.num_batches_tracked += 1\n",
    "\n",
    "        residual = x\n",
    "        residual = self.conv4(residual)\n",
    "        residual = self.bn4(residual)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block):\n",
    "        self.inplanes = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer1 = BasicBlock(3,64, 2)\n",
    "        self.layer2 = BasicBlock(64,128, 2)\n",
    "        self.layer3 = BasicBlock(128,256, 2)\n",
    "        self.layer4 = BasicBlock(256,512, 1)\n",
    "#         UNCOMMENT ONLY FOR COVARIANCE POOLING\n",
    "#         self.layer5 = nn.Conv2d(512, 128, kernel_size=1, stride=1)\n",
    "\n",
    "\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # TODO: is this fine?\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "#         UNCOMMENT ONLY FOR COVARIANCE POOLING\n",
    "#         x = self.layer5(x)\n",
    "        return x/math.sqrt(network_width)\n",
    "\n",
    "\n",
    "def resnet12():\n",
    "    \"\"\"Constructs a ResNet-12 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Make Models\n",
    "    \n",
    "# models = [PROTO(network_width).cuda() for i in range(ensemble)]\n",
    "models = [resnet12().cuda() for i in range(ensemble)]\n",
    "\n",
    "expander = avgpool()\n",
    "if localizing:\n",
    "    if fewshot_local:\n",
    "        expander = fsCL if covariance_pooling else fsL\n",
    "    else:\n",
    "        expander = pCL() if covariance_pooling else pL()\n",
    "elif covariance_pooling:\n",
    "    expander = covapool\n",
    "expanders = [expander for _ in range(ensemble)]\n",
    "\n",
    "# Load saved parameters\n",
    "model_state = torch.load(model)\n",
    "for i in range(ensemble):\n",
    "    models[i].load_state_dict(model_state[i])\n",
    "    models[i].eval()\n",
    "    # Zero out the bias on the final layer, since it doesn't do anything\n",
    "    # models[i].process[-1].layers[-1].bias.data.zero_()\n",
    "\n",
    "# Load additional parameters for parametric localizer models\n",
    "if localizing and not fewshot_local:\n",
    "    fbcentroids = torch.load(model[:model.rfind('.')]+'_localizers'+model[model.rfind('.'):])\n",
    "    for i in range(ensemble):\n",
    "        expanders[i].centroids.data = fbcentroids[i]\n",
    "        expanders[i].cuda()\n",
    "\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                    EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/1b501b470586254a8e39b42c6b495308.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/41dd46d7cdd762ea0810a25ec5d6f11b.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/ea3b096eb00b458ff5341b724b5972fe.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/a94a5f63ceb7fb8f2b8de13081fc6d2f.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/7fc5a03cc8147a2bb7a40fdb52760d3f.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/fb8a65c03eb6cd43679caf2c61ec8614.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/84d1f128f17f1363965ae5281204f28a.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/3ec5eb2cb2fd6fc2e14264e0b508e108.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/e8ff93b66f5bfa15d161f21dbe85e1f7.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/99c8ff1a5e16d5bf3b6b2d65293c182e.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/9f2963f0f701d898ae7ee2b1db325f43.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/9c8dfbad758f7e78d10acf5fafb5b631.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/e33232e2f6c73a805479a6b0154d4fc6.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/0e8b4c79d866f8f9cb2e1bdeb7d36c7d.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/3f6194c8e8571ae0edf43b8f76fb57f7.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/47f067813b480cce97c0bf8b668ae05a.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/b74d26bed63be89d2b69e7c9e768d2ca.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/7cfa0186af45070cfd89503a178859c7.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/74cc968e3bed603dc7449fce202be158.bmp\n",
      "/\n",
      "/\n",
      "/data/dww78/mini_inat_shrunk/repr/112/059f13c6439b4a1cf4030ad332089216.bmp\n",
      "/\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (512) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e38128a44fa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Accumulate foreground/background prototypes, if using\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     fbcentroids = (accumulateFB(models, repr_loader, way, network_width, ngiv, bsize)\n\u001b[0m\u001b[1;32m      7\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0minclude_masks\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                    [None]*ensemble)\n",
      "\u001b[0;32m/data/db638/fewshotlocal/helpful_files/testing.py\u001b[0m in \u001b[0;36maccumulateFB\u001b[0;34m(models, loader, way, network_width, ngiven, bsize)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# b 64 10 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mfbcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfbpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mslack\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mbsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (512) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "acclist = []\n",
    "pcacclist = []\n",
    "alldispacc = np.zeros(way)\n",
    "for r in range(n_trials):\n",
    "    # Accumulate foreground/background prototypes, if using\n",
    "    fbcentroids = (accumulateFB(models, repr_loader, way, network_width, ngiv, bsize)\n",
    "                   if include_masks else \n",
    "                   [None]*ensemble)\n",
    "    # Accumulate category prototypes\n",
    "    centroids, counts = accumulate(models, repr_loader, expanders, \n",
    "                                   fbcentroids, way, d)\n",
    "    # Score the models\n",
    "    allacc, dispacc, perclassacc = score(k, centroids, fbcentroids, models, \n",
    "                                         query_loader, expanders, way)\n",
    "    # Record statistics\n",
    "    acclist = acclist+allacc\n",
    "    pcacclist = pcacclist+list(perclassacc)\n",
    "    alldispacc += dispacc\n",
    "\n",
    "# Aggregate collected statistics\n",
    "accs = sum(acclist)/n_trials/ensemble\n",
    "pcaccs = sum(pcacclist)/n_trials/ensemble\n",
    "alldispacc = alldispacc/n_trials\n",
    "confs = 1.96*np.sqrt(np.var(acclist)/n_trials/ensemble)\n",
    "pcconfs = 1.96*np.sqrt(np.var(pcacclist)/n_trials/ensemble)\n",
    "\n",
    "# Report\n",
    "print(\"Accuracies and 95% confidence intervals\")\n",
    "print(\"Mean accuracy: \\t\\t%.2f \\t+/- %.2f\" % (accs*100, confs*100))\n",
    "print(\"Per-class accuracy: \\t%.f \\t+/- %.2f\" % (pcaccs*100, pcconfs*100))\n",
    "logcounts = [np.log10(c) for c in counts]\n",
    "pl.figure()\n",
    "pl.axhline(0,color='k')\n",
    "pl.scatter(counts, dispacc*100, s=4)\n",
    "z = np.polyfit(logcounts, np.array(dispacc)*100, 1)\n",
    "p = np.poly1d(z)\n",
    "pl.plot([min(counts),max(counts)], [p(min(logcounts)),p(max(logcounts))], \"r--\")\n",
    "pl.ylim([0,100])\n",
    "pl.xlabel('# Reference Images')\n",
    "pl.ylabel('Percentage Points')\n",
    "pl.xscale('log')\n",
    "pl.title('Per-Class Top-%d Accuracy' % k)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/db638/repr/1027/bafc96ec591660f9dd262d54d8ec1d48.bmp: broken symbolic link to ../test/1027/bafc96ec591660f9dd262d54d8ec1d48.bmp\r\n"
     ]
    }
   ],
   "source": [
    "!file /data/db638/repr/1027/bafc96ec591660f9dd262d54d8ec1d48.bmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU space\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.session.delete();\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
