{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import NLLLoss\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "from helpful_files.networks import PROTO, avgpool, covapool, pL, pCL, fsL, fsCL, fbpredict\n",
    "from helpful_files.testing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Important Values\n",
    "\n",
    "# General settings\n",
    "datapath = '/data/dww78/mini_inat_shrunk/'                      # The location of your train, test, repr, and query folders. Make sure it ends in '/'!\n",
    "model = 'modified-proto-Train-111.pth'               # What model do you wish to evaluate, and where is it saved?\n",
    "gpu = 2                             # What gpu do you wish to run on?\n",
    "workers = 1                         # Number of cpu worker processes to use for data loading\n",
    "verbosity = 10                      # How many categories in between status updates \n",
    "ensemble = 4                        # How many models to evaluate in parallel\n",
    "k = 5                               # Evaluate top-k accuracy. Typically 1 or 5. \n",
    "torch.cuda.set_device(gpu) \n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Model characteristics\n",
    "covariance_pooling = True           # Did your model use covariance pooling?\n",
    "localizing = True                   # Did your model use localization?\n",
    "fewshot_local = True                # If you used localization: few-shot, or parametric? Few-shot if True, param if False\n",
    "network_width = 64                  # Number of channels at every layer of the network\n",
    "\n",
    "# Batch construction\n",
    "bsize = 64                          # Batch size\n",
    "boxes_available = 10                # Percentage of images with bounding boxes available (few-shot localization models only)\n",
    "include_masks = (localizing         # Include or ignore the bounding box annotations?\n",
    "                 and fewshot_local)\n",
    "n_trials = (10                      # Number of trials (few-shot localization models only)\n",
    "            if include_masks else 1)\n",
    "\n",
    "\n",
    "# Calculate embedding size based on model setup\n",
    "d = (network_width if not \n",
    "     covariance_pooling else\n",
    "     network_width**2)\n",
    "if localizing and not covariance_pooling:\n",
    "    d = network_width*2\n",
    "assert n_trials == 1 or include_masks, (\"Repeated trials will yield repeated identical results under this configuration.\"+\n",
    "                                        \"Please set ntrials to 1 or use a few-shot localizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load Testing Data\n",
    "\n",
    "d_boxes = torch.load(datapath + 'box_coords.pth')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4905, 0.4961, 0.4330],std=[0.1737, 0.1713, 0.1779])\n",
    "    ])\n",
    "\n",
    "repr_dataset = datasets.ImageFolder(\n",
    "    datapath+'repr', \n",
    "    loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "query_dataset = datasets.ImageFolder(\n",
    "    datapath+'query',\n",
    "    loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "repr_loader = torch.utils.data.DataLoader(\n",
    "    repr_dataset, \n",
    "    batch_sampler = OrderedSampler(repr_dataset, bsize),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "query_loader = torch.utils.data.DataLoader(\n",
    "    query_dataset,\n",
    "    batch_sampler = OrderedSampler(query_dataset, bsize),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "way = len(repr_dataset.classes)\n",
    "\n",
    "# Determine number of images with bounding boxes per-class\n",
    "catsizes = torch.LongTensor(np.array([t[1] for t in repr_dataset.imgs])).bincount().float()\n",
    "ngiv = (catsizes*boxes_available//100)\n",
    "for i in range(ngiv.size(0)):\n",
    "    if ngiv[i] == 0:\n",
    "        ngiv[i] = 1\n",
    "ngiv = ngiv.long().tolist()\n",
    "\n",
    "print('Data loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Make Models\n",
    "    \n",
    "models = [PROTO(network_width).cuda() for i in range(ensemble)]\n",
    "expander = avgpool()\n",
    "if localizing:\n",
    "    if fewshot_local:\n",
    "        expander = fsCL if covariance_pooling else fsL\n",
    "    else:\n",
    "        expander = pCL() if covariance_pooling else pL()\n",
    "elif covariance_pooling:\n",
    "    expander = covapool\n",
    "expanders = [expander for _ in range(ensemble)]\n",
    "\n",
    "# Load saved parameters\n",
    "model_state = torch.load(model)\n",
    "for i in range(ensemble):\n",
    "    models[i].load_state_dict(model_state[i])\n",
    "    models[i].eval()\n",
    "    # Zero out the bias on the final layer, since it doesn't do anything\n",
    "    models[i].process[-1].layers[-1].bias.data.zero_()\n",
    "\n",
    "# Load additional parameters for parametric localizer models\n",
    "if localizing and not fewshot_local:\n",
    "    fbcentroids = torch.load(model[:model.rfind('.')]+'_localizers'+model[model.rfind('.'):])\n",
    "    for i in range(ensemble):\n",
    "        expanders[i].centroids.data = fbcentroids[i]\n",
    "        expanders[i].cuda()\n",
    "\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                    EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You don't want to append your tensor to total _accum \n",
    "# until after you've completed the category, so that should\n",
    "# happen in the upper if then block and you should be concatenating\n",
    "# to running in the lower for loop\n",
    "# total _accum list should have 227 entries\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# b is actually a different number for every class \n",
    "# esize = 4; total accum should be 4 x 227 x b x d\n",
    "# 227 classes, for each categories size of embeddings is of size b x d\n",
    "# list of length 4 containing lists of length 227, each element of list will be a b x d tensor\n",
    "\n",
    "# total accum: [[[bxd],..,[b227xd]], [[*bxd],...,[*b227xd]], [[**bxd],...[**b227xd]], [[***bxd],...,[***b227xd]]]\n",
    "# [4,227,b,d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate category prototypes\n",
    "def accumulate(models, loader, expanders, bcentroids, way, d):\n",
    "    esize = len(models)\n",
    "    total_accum = [[],[],[],[]]\n",
    "    catindex = 0\n",
    "    lastcat = -1\n",
    "    running = [torch.zeros(0, d)]*4\n",
    "    progress = torch.zeros(1, way)\n",
    "    for i, ((inp,_), cat) in enumerate(loader):\n",
    "        catindex = cat[0]\n",
    "\n",
    "        # Moving to another category\n",
    "        if catindex != lastcat: \n",
    "            if i != 0:\n",
    "                for j in range(esize):\n",
    "                    total_accum[j].append(running[j]) # Write the values\n",
    "            lastcat = catindex # Record the current category\n",
    "            running = [torch.zeros(0, d)]*4\n",
    "            progress[0, lastcat] = 1\n",
    "            # Plot progress\n",
    "#             display.clear_output(wait=True)\n",
    "#             pl.figure(figsize=(20,1))\n",
    "#             pl.imshow(progress.numpy(), cmap='Greys')\n",
    "#             pl.title(\"Accumulating category prototypes:\")\n",
    "#             pl.xticks([])\n",
    "#             pl.yticks([])\n",
    "#             pl.show()\n",
    "#             sleep(.01)\n",
    "\n",
    "        # Continue accumulating\n",
    "        inp = inp.cuda()\n",
    "        with torch.no_grad():\n",
    "            for j in range(esize):\n",
    "                out = models[j](inp) # b 64 10 10\n",
    "                out = expanders[j](out, bcentroids[j], None) # b d\n",
    "                running[j] = torch.cat((running[j], out.cpu()),0)\n",
    "#                 running[j] += out.sum(0) [1xd] # Accumulate prototypes\n",
    "\n",
    "    # Record last category\n",
    "    for j in range(esize):\n",
    "        total_accum[j].append(running[j])\n",
    "#     print(len(total_accum))\n",
    "#     print(len(total_accum[0]))\n",
    "#     for i in total_accum[0]:\n",
    "#         print(len(i))\n",
    "    return total_accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "# TODO: write predict functions using SVM (sci-kit), L2-norm nearest-neighbors, arg to predict fxn\n",
    "# will be list of all tensors rather than centroids\n",
    "def predict(centroids, query):\n",
    "    # centroids and query must be broadcastable!\n",
    "    \n",
    "    #Import svm model\n",
    "    from sklearn import svm\n",
    "\n",
    "    #Create a svm Classifier\n",
    "    clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "    #Train the model using the training sets\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    distmat = torch.sum((centroids-query)**2,-1).neg().view(-1, centroids.size(-2))\n",
    "    return F.log_softmax(distmat, dim=-1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(k, centroids, bcentroids, models, loader, expanders, way):\n",
    "    esize = len(models)\n",
    "    right = [0]*esize\n",
    "    allright = [0]*esize\n",
    "    perclassacc = np.array([[0.]*way for _ in range(esize)])\n",
    "    catindex = 0\n",
    "    lastcat = -1\n",
    "    count = 0\n",
    "    allcount = 0\n",
    "    progress = torch.zeros(1, way)\n",
    "    for i, ((inp,_), cat) in enumerate(loader):\n",
    "        catindex = cat[0]\n",
    "        if catindex != lastcat: # We're about to move to another category\n",
    "            # Write the values\n",
    "            if i!= 0:\n",
    "                allcount += count\n",
    "                for j in range(esize):\n",
    "                    allright[j] += right[j] \n",
    "                    perclassacc[j, lastcat] = right[j]/count\n",
    "            lastcat = catindex # Record the current category\n",
    "            count = 0 # Reset divisor\n",
    "            right = [0]*esize # Reset accumulator\n",
    "            progress[0, lastcat] = 1\n",
    "            # Plot progress\n",
    "#             display.clear_output(wait=True)\n",
    "#             pl.figure(figsize=(20,1))\n",
    "#             pl.imshow(progress.numpy(), cmap='Greys')\n",
    "#             pl.title(\"Accumulating accuracy scores:\")\n",
    "#             pl.xticks([])\n",
    "#             pl.yticks([])\n",
    "#             pl.show()\n",
    "#             sleep(.01)\n",
    "\n",
    "        # Predict\n",
    "        inp = inp.cuda()\n",
    "        targ = cat.cuda()\n",
    "        with torch.no_grad():\n",
    "            for j in range(esize):\n",
    "                out = models[j](inp)\n",
    "                out = expanders[j](out, bcentroids[j], None)\n",
    "                out = predict(centroids[j].unsqueeze(0), out.unsqueeze(1))\n",
    "                _, pred = out.topk(k, 1, True, True)\n",
    "                pred = pred.t()\n",
    "                right[j] += pred.eq(targ.view(1, -1).expand_as(pred))[:k].view(-1).sum(0, keepdim=True).float().item()\n",
    "        count += inp.size(0)\n",
    "\n",
    "    # Record last category\n",
    "    allcount += count\n",
    "    for j in range(esize):\n",
    "        allright[j] += right[j]\n",
    "        perclassacc[j, catindex] = right[j]/count\n",
    "\n",
    "    # Final reporting / recording\n",
    "    allacc = [r/allcount for r in allright]\n",
    "    \n",
    "    return allacc, np.mean(perclassacc, axis=0), np.mean(perclassacc, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at end of accum\n",
      "4\n",
      "227\n",
      "50\n",
      "27\n",
      "20\n",
      "14\n",
      "68\n",
      "11\n",
      "180\n",
      "16\n",
      "27\n",
      "10\n",
      "18\n",
      "10\n",
      "81\n",
      "49\n",
      "15\n",
      "20\n",
      "15\n",
      "21\n",
      "65\n",
      "109\n",
      "10\n",
      "49\n",
      "22\n",
      "24\n",
      "37\n",
      "38\n",
      "56\n",
      "38\n",
      "39\n",
      "12\n",
      "104\n",
      "19\n",
      "17\n",
      "138\n",
      "48\n",
      "11\n",
      "21\n",
      "168\n",
      "31\n",
      "154\n",
      "38\n",
      "24\n",
      "41\n",
      "51\n",
      "16\n",
      "20\n",
      "75\n",
      "40\n",
      "30\n",
      "10\n",
      "11\n",
      "186\n",
      "43\n",
      "64\n",
      "40\n",
      "124\n",
      "16\n",
      "10\n",
      "27\n",
      "19\n",
      "24\n",
      "24\n",
      "27\n",
      "32\n",
      "56\n",
      "14\n",
      "12\n",
      "13\n",
      "109\n",
      "14\n",
      "57\n",
      "18\n",
      "39\n",
      "10\n",
      "30\n",
      "71\n",
      "10\n",
      "12\n",
      "27\n",
      "77\n",
      "12\n",
      "13\n",
      "22\n",
      "16\n",
      "12\n",
      "157\n",
      "12\n",
      "33\n",
      "56\n",
      "31\n",
      "13\n",
      "193\n",
      "33\n",
      "29\n",
      "52\n",
      "41\n",
      "28\n",
      "13\n",
      "11\n",
      "48\n",
      "32\n",
      "38\n",
      "11\n",
      "16\n",
      "18\n",
      "11\n",
      "108\n",
      "61\n",
      "142\n",
      "157\n",
      "26\n",
      "17\n",
      "92\n",
      "39\n",
      "10\n",
      "27\n",
      "11\n",
      "11\n",
      "29\n",
      "74\n",
      "25\n",
      "12\n",
      "19\n",
      "39\n",
      "12\n",
      "46\n",
      "62\n",
      "22\n",
      "131\n",
      "80\n",
      "75\n",
      "32\n",
      "23\n",
      "46\n",
      "20\n",
      "91\n",
      "90\n",
      "22\n",
      "10\n",
      "51\n",
      "22\n",
      "11\n",
      "30\n",
      "36\n",
      "20\n",
      "125\n",
      "11\n",
      "17\n",
      "12\n",
      "41\n",
      "127\n",
      "11\n",
      "26\n",
      "34\n",
      "64\n",
      "12\n",
      "57\n",
      "92\n",
      "34\n",
      "27\n",
      "41\n",
      "15\n",
      "19\n",
      "43\n",
      "20\n",
      "15\n",
      "10\n",
      "38\n",
      "21\n",
      "119\n",
      "17\n",
      "17\n",
      "37\n",
      "11\n",
      "35\n",
      "20\n",
      "97\n",
      "16\n",
      "33\n",
      "38\n",
      "10\n",
      "25\n",
      "31\n",
      "43\n",
      "107\n",
      "88\n",
      "11\n",
      "11\n",
      "48\n",
      "18\n",
      "15\n",
      "45\n",
      "59\n",
      "23\n",
      "23\n",
      "60\n",
      "19\n",
      "17\n",
      "57\n",
      "57\n",
      "38\n",
      "33\n",
      "26\n",
      "24\n",
      "15\n",
      "14\n",
      "66\n",
      "25\n",
      "25\n",
      "37\n",
      "21\n",
      "51\n",
      "76\n",
      "30\n",
      "29\n",
      "41\n",
      "13\n",
      "40\n",
      "40\n",
      "27\n",
      "11\n",
      "41\n",
      "32\n",
      "56\n",
      "27\n",
      "14\n",
      "11\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-04c83c692ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                    fbcentroids, way, d)\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Score the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     allacc, dispacc, perclassacc = score(k, SVM_points, fbcentroids, models, \n\u001b[0m\u001b[1;32m     15\u001b[0m                                          query_loader, expanders, way)\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Record statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-87457d1ec6ae>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(k, centroids, bcentroids, models, loader, expanders, way)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpanders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "acclist = []\n",
    "pcacclist = []\n",
    "alldispacc = np.zeros(way)\n",
    "for r in range(n_trials):\n",
    "    # Accumulate foreground/background prototypes, if using\n",
    "    fbcentroids = (accumulateFB(models, repr_loader, way, network_width, ngiv, bsize)\n",
    "                   if include_masks else \n",
    "                   [None]*ensemble)\n",
    "    # Accumulate category prototypes\n",
    "\n",
    "    SVM_points = accumulate(models, repr_loader, expanders, \n",
    "                                   fbcentroids, way, d)\n",
    "    # Score the models\n",
    "    allacc, dispacc, perclassacc = score(k, SVM_points, fbcentroids, models, \n",
    "                                         query_loader, expanders, way)\n",
    "    # Record statistics\n",
    "    acclist = acclist+allacc\n",
    "    pcacclist = pcacclist+list(perclassacc)\n",
    "    alldispacc += dispacc\n",
    "\n",
    "# Aggregate collected statistics\n",
    "accs = sum(acclist)/n_trials/ensemble\n",
    "pcaccs = sum(pcacclist)/n_trials/ensemble\n",
    "alldispacc = alldispacc/n_trials\n",
    "confs = 1.96*np.sqrt(np.var(acclist)/n_trials/ensemble)\n",
    "pcconfs = 1.96*np.sqrt(np.var(pcacclist)/n_trials/ensemble)\n",
    "\n",
    "# Report\n",
    "print(\"Accuracies and 95% confidence intervals\")\n",
    "print(\"Mean accuracy: \\t\\t%.2f \\t+/- %.2f\" % (accs*100, confs*100))\n",
    "print(\"Per-class accuracy: \\t%.f \\t+/- %.2f\" % (pcaccs*100, pcconfs*100))\n",
    "logcounts = [np.log10(c) for c in counts]\n",
    "pl.figure()\n",
    "pl.axhline(0,color='k')\n",
    "pl.scatter(counts, dispacc*100, s=4)\n",
    "z = np.polyfit(logcounts, np.array(dispacc)*100, 1)\n",
    "p = np.poly1d(z)\n",
    "pl.plot([min(counts),max(counts)], [p(min(logcounts)),p(max(logcounts))], \"r--\")\n",
    "pl.ylim([0,100])\n",
    "pl.xlabel('# Reference Images')\n",
    "pl.ylabel('Percentage Points')\n",
    "pl.xscale('log')\n",
    "pl.title('Per-Class Top-%d Accuracy' % k)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fewshot",
   "language": "python",
   "name": "fewshot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
