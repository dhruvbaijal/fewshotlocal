{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import NLLLoss\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from helpful_files.networks import *\n",
    "from helpful_files.training import *\n",
    "from helpful_files.testing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Important Values\n",
    "\n",
    "# General settings\n",
    "datapath = '/data/dww78/mini_inat_shrunk/'                     # The location of your train, test, repr, and query folders. Make sure it ends in '/'!\n",
    "model = 'ResNet-110.pth'  # What model do you wish to evaluate, and where is it saved?\n",
    "gpu = 2                             # What gpu do you wish to run on?\n",
    "workers = 1                         # Number of cpu worker processes to use for data loading\n",
    "verbosity = 10                      # How many categories in between status updates \n",
    "ensemble = 4                        # How many models to evaluate in parallel\n",
    "k = 1                               # Evaluate top-k accuracy. Typically 1 or 5. \n",
    "torch.cuda.set_device(gpu) \n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Model characteristics\n",
    "covariance_pooling = False           # Did your model use covariance pooling?\n",
    "localizing = True                   # Did your model use localization?\n",
    "fewshot_local = True                # If you used localization: few-shot, or parametric? Few-shot if True, param if False\n",
    "network_width = 512                  # Number of channels at every layer of the network\n",
    "trainshot = 5 \n",
    "testshot = 15 \n",
    "# Batch construction\n",
    "bsize = 64                          # Batch size\n",
    "boxes_available = 10                # Percentage of images with bounding boxes available (few-shot localization models only)\n",
    "include_masks = (localizing         # Include or ignore the bounding box annotations?\n",
    "                 and fewshot_local)\n",
    "n_trials = (10                      # Number of trials (few-shot localization models only)\n",
    "            if include_masks else 1)\n",
    "\n",
    "augmentation_flipping = False\n",
    "folding = False\n",
    "# Calculate embedding size based on model setup\n",
    "d = (network_width if not \n",
    "     covariance_pooling else\n",
    "     network_width**2)\n",
    "if localizing and not covariance_pooling:\n",
    "    d = network_width*2\n",
    "assert n_trials == 1 or include_masks, (\"Repeated trials will yield repeated identical results under this configuration.\"+\n",
    "                                        \"Please set ntrials to 1 or use a few-shot localizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load Testing Data\n",
    "\n",
    "d_boxes = torch.load('/data/db638/github/fewshotlocal/helpful_files/box_coords.pth')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4905, 0.4961, 0.4330],std=[0.1737, 0.1713, 0.1779])\n",
    "    ])\n",
    "\n",
    "if folding:\n",
    "    # Batch folding has no reference/query distinction\n",
    "    shots = [trainshot+testshot]\n",
    "else:\n",
    "    # Standard setup\n",
    "    shots = [trainshot, testshot]\n",
    "if localizing and fewshot_local and not folding:\n",
    "    # Unfolded prototype localizers need another set of reference images to inform foreground/background predictions\n",
    "    shots = [trainshot, trainshot, testshot-trainshot]\n",
    "    \n",
    "way = 5\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    datapath+'test', \n",
    "    loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_sampler = ProtoSampler(test_dataset, way, shots),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "\n",
    "# repr_dataset = datasets.ImageFolder(\n",
    "#     datapath+'repr', \n",
    "#     loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "# query_dataset = datasets.ImageFolder(\n",
    "#     datapath+'query',\n",
    "#     loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "# repr_loader = torch.utils.data.DataLoader(\n",
    "#     repr_dataset, \n",
    "#     batch_sampler = OrderedSampler(repr_dataset, bsize),\n",
    "#     num_workers = workers,\n",
    "#     pin_memory = True)\n",
    "# query_loader = torch.utils.data.DataLoader(\n",
    "#     query_dataset,\n",
    "#     batch_sampler = OrderedSampler(query_dataset, bsize),\n",
    "#     num_workers = workers,\n",
    "#     pin_memory = True)\n",
    "\n",
    "\n",
    "# Determine number of images with bounding boxes per-class\n",
    "catsizes = torch.LongTensor(np.array([t[1] for t in test_dataset.imgs])).bincount().float()\n",
    "ngiv = (catsizes*boxes_available//100)\n",
    "for i in range(ngiv.size(0)):\n",
    "    if ngiv[i] == 0:\n",
    "        ngiv[i] = 1\n",
    "ngiv = ngiv.long().tolist()\n",
    "\n",
    "print('Data loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d_boxes = dict()\n",
    "for key, value in d_boxes.items():\n",
    "    new_key = \"/data/dww78/mini_inat_shrunk/\"+key[3:]\n",
    "    new_d_boxes[new_key] = value\n",
    "\n",
    "d_boxes = new_d_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv3 = conv3x3(planes, planes)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "      \n",
    "        self.conv4 = conv3x3(inplanes, planes)\n",
    "        self.bn4 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(stride)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.num_batches_tracked = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.num_batches_tracked += 1\n",
    "\n",
    "        residual = x\n",
    "        residual = self.conv4(residual)\n",
    "        residual = self.bn4(residual)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block):\n",
    "        self.inplanes = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer1 = BasicBlock(3,64, 2)\n",
    "        self.layer2 = BasicBlock(64,128, 2)\n",
    "        self.layer3 = BasicBlock(128,256, 2)\n",
    "        self.layer4 = BasicBlock(256,512, 1)\n",
    "#         UNCOMMENT ONLY FOR COVARIANCE POOLING\n",
    "#         self.layer5 = nn.Conv2d(512, 128, kernel_size=1, stride=1)\n",
    "\n",
    "\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # TODO: is this fine?\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "#         UNCOMMENT ONLY FOR COVARIANCE POOLING\n",
    "#         x = self.layer5(x)\n",
    "        return x/math.sqrt(network_width)\n",
    "\n",
    "\n",
    "def resnet12():\n",
    "    \"\"\"Constructs a ResNet-12 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Make Models\n",
    "\n",
    "\n",
    "# models = [resnet12().cuda() for i in range(ensemble)]\n",
    "\n",
    "\n",
    "models = [Network(network_width, folding, covariance_pooling,\n",
    "                  localizing, fewshot_local, shots).cuda() for i in range(ensemble)]\n",
    "\n",
    "# Load saved parameters\n",
    "model_state = torch.load(model)\n",
    "\n",
    "for model in models:\n",
    "    model.encode = resnet12().cuda()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(ensemble):\n",
    "    models[i].encode.load_state_dict(model_state[i])\n",
    "    models[i].encode.eval()\n",
    "    # Zero out the bias on the final layer, since it doesn't do anything\n",
    "#     models[i].encode.process[-1].layers[-1].bias.data.zero_()\n",
    "\n",
    "# Load additional parameters for parametric localizer models\n",
    "# if localizing and not fewshot_local:\n",
    "#     fbcentroids = torch.load(model[:model.rfind('.')]+'_localizers'+model[model.rfind('.'):])\n",
    "#     for i in range(ensemble):\n",
    "#         expanders[i].centroids.data = fbcentroids[i]\n",
    "#         expanders[i].cuda()\n",
    "\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                    EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodic_eval(test_loader, models, way, shots, verbosity):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    nqueries = shots[-1]\n",
    "    ensemble = len(models)\n",
    "    targ = torch.LongTensor([i//nqueries for i in range(nqueries*way)]).cuda()\n",
    "#     print(targ.shape)\n",
    "    acctracker = [[] for i in range(ensemble)]\n",
    "    print(\"Training images covered this round:\")\n",
    "    for i, ((inp, masks), _) in enumerate(test_loader):\n",
    "        inp = inp.cuda()\n",
    "        masks = masks.cuda()\n",
    "        for j in range(ensemble):\n",
    "            # Predict, step\n",
    "            out = models[j](inp, masks)\n",
    "#             if i == 0:\n",
    "#                 print(out.shape)\n",
    "#                 print(out)\n",
    "            _,bins = torch.max(out,1)\n",
    "#             print(bins.shape)\n",
    "            acc = torch.sum(torch.eq(bins,targ)).item()/nqueries/way\n",
    "            acctracker[j].append(acc)\n",
    "        if i%verbosity == 0:\n",
    "            print('%d of approx. 192270'%(i*way*sum(shots)))\n",
    "    all_acc = []\n",
    "    for i in acctracker:\n",
    "        for j in i:\n",
    "            all_acc.append(j)\n",
    "    mean_acc = sum(all_acc)/len(all_acc)\n",
    "    confs = 1.96*np.sqrt(np.var(all_acc)/len(all_acc))\n",
    "    return mean_acc, confs\n",
    "#     return [L/(i+1) for L in acctracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images covered this round:\n",
      "0 of approx. 192270\n",
      "1000 of approx. 192270\n",
      "2000 of approx. 192270\n",
      "3000 of approx. 192270\n",
      "4000 of approx. 192270\n",
      "5000 of approx. 192270\n",
      "6000 of approx. 192270\n",
      "7000 of approx. 192270\n",
      "8000 of approx. 192270\n",
      "9000 of approx. 192270\n",
      "10000 of approx. 192270\n",
      "11000 of approx. 192270\n",
      "12000 of approx. 192270\n",
      "13000 of approx. 192270\n",
      "14000 of approx. 192270\n",
      "15000 of approx. 192270\n",
      "16000 of approx. 192270\n",
      "17000 of approx. 192270\n",
      "18000 of approx. 192270\n",
      "19000 of approx. 192270\n",
      "20000 of approx. 192270\n",
      "21000 of approx. 192270\n",
      "22000 of approx. 192270\n",
      "23000 of approx. 192270\n",
      "24000 of approx. 192270\n",
      "25000 of approx. 192270\n",
      "26000 of approx. 192270\n",
      "27000 of approx. 192270\n",
      "28000 of approx. 192270\n",
      "29000 of approx. 192270\n",
      "30000 of approx. 192270\n",
      "31000 of approx. 192270\n",
      "32000 of approx. 192270\n",
      "33000 of approx. 192270\n",
      "34000 of approx. 192270\n",
      "35000 of approx. 192270\n",
      "36000 of approx. 192270\n",
      "37000 of approx. 192270\n",
      "38000 of approx. 192270\n",
      "39000 of approx. 192270\n",
      "40000 of approx. 192270\n",
      "41000 of approx. 192270\n",
      "42000 of approx. 192270\n",
      "43000 of approx. 192270\n",
      "44000 of approx. 192270\n",
      "87.79524886877923\n",
      "0.3168685995682712\n"
     ]
    }
   ],
   "source": [
    "mean_acc, confs = episodic_eval(test_loader, models, way, shots, verbosity)\n",
    "print(mean_acc*100)\n",
    "print(confs*100)\n",
    "# accs = sum(acclist)/n_trials/ensemble\n",
    "# confs = 1.96*np.sqrt(np.var(acclist)/n_trials/ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Jupyter.notebook.session.delete();\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fewshot",
   "language": "python",
   "name": "fewshot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
