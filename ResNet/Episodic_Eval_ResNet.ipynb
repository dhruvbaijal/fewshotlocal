{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import NLLLoss\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from helpful_files.networks import *\n",
    "from helpful_files.training import *\n",
    "from helpful_files.testing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Important Values\n",
    "\n",
    "# General settings\n",
    "datapath = '/data/dww78/mini_inat_shrunk/'                     # The location of your train, test, repr, and query folders. Make sure it ends in '/'!\n",
    "model = 'ResNet-100.pth'  # What model do you wish to evaluate, and where is it saved?\n",
    "gpu = 2                             # What gpu do you wish to run on?\n",
    "workers = 1                         # Number of cpu worker processes to use for data loading\n",
    "verbosity = 10                      # How many categories in between status updates \n",
    "ensemble = 4                        # How many models to evaluate in parallel\n",
    "k = 1                               # Evaluate top-k accuracy. Typically 1 or 5. \n",
    "torch.cuda.set_device(gpu) \n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Model characteristics\n",
    "covariance_pooling = False           # Did your model use covariance pooling?\n",
    "localizing = False                   # Did your model use localization?\n",
    "fewshot_local = False                # If you used localization: few-shot, or parametric? Few-shot if True, param if False\n",
    "network_width = 512                  # Number of channels at every layer of the network\n",
    "trainshot = 5 \n",
    "testshot = 15 \n",
    "# Batch construction\n",
    "bsize = 64                          # Batch size\n",
    "boxes_available = 10                # Percentage of images with bounding boxes available (few-shot localization models only)\n",
    "include_masks = (localizing         # Include or ignore the bounding box annotations?\n",
    "                 and fewshot_local)\n",
    "n_trials = (10                      # Number of trials (few-shot localization models only)\n",
    "            if include_masks else 1)\n",
    "\n",
    "augmentation_flipping = False\n",
    "folding = False\n",
    "# Calculate embedding size based on model setup\n",
    "d = (network_width if not \n",
    "     covariance_pooling else\n",
    "     network_width**2)\n",
    "if localizing and not covariance_pooling:\n",
    "    d = network_width*2\n",
    "assert n_trials == 1 or include_masks, (\"Repeated trials will yield repeated identical results under this configuration.\"+\n",
    "                                        \"Please set ntrials to 1 or use a few-shot localizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load Testing Data\n",
    "\n",
    "d_boxes = torch.load('/data/db638/github/fewshotlocal/helpful_files/box_coords.pth')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4905, 0.4961, 0.4330],std=[0.1737, 0.1713, 0.1779])\n",
    "    ])\n",
    "\n",
    "if folding:\n",
    "    # Batch folding has no reference/query distinction\n",
    "    shots = [trainshot+testshot]\n",
    "else:\n",
    "    # Standard setup\n",
    "    shots = [trainshot, testshot]\n",
    "if localizing and fewshot_local and not folding:\n",
    "    # Unfolded prototype localizers need another set of reference images to inform foreground/background predictions\n",
    "    shots = [trainshot, trainshot, testshot-trainshot]\n",
    "    \n",
    "way = 5\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    datapath+'test', \n",
    "    loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_sampler = ProtoSampler(test_dataset, way, shots),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "\n",
    "# repr_dataset = datasets.ImageFolder(\n",
    "#     datapath+'repr', \n",
    "#     loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "# query_dataset = datasets.ImageFolder(\n",
    "#     datapath+'query',\n",
    "#     loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "# repr_loader = torch.utils.data.DataLoader(\n",
    "#     repr_dataset, \n",
    "#     batch_sampler = OrderedSampler(repr_dataset, bsize),\n",
    "#     num_workers = workers,\n",
    "#     pin_memory = True)\n",
    "# query_loader = torch.utils.data.DataLoader(\n",
    "#     query_dataset,\n",
    "#     batch_sampler = OrderedSampler(query_dataset, bsize),\n",
    "#     num_workers = workers,\n",
    "#     pin_memory = True)\n",
    "\n",
    "\n",
    "# Determine number of images with bounding boxes per-class\n",
    "catsizes = torch.LongTensor(np.array([t[1] for t in test_dataset.imgs])).bincount().float()\n",
    "ngiv = (catsizes*boxes_available//100)\n",
    "for i in range(ngiv.size(0)):\n",
    "    if ngiv[i] == 0:\n",
    "        ngiv[i] = 1\n",
    "ngiv = ngiv.long().tolist()\n",
    "\n",
    "print('Data loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d_boxes = dict()\n",
    "for key, value in d_boxes.items():\n",
    "    new_key = \"/data/dww78/mini_inat_shrunk/\"+key[3:]\n",
    "    new_d_boxes[new_key] = value\n",
    "\n",
    "d_boxes = new_d_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv3 = conv3x3(planes, planes)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "      \n",
    "        self.conv4 = conv3x3(inplanes, planes)\n",
    "        self.bn4 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(stride)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.num_batches_tracked = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.num_batches_tracked += 1\n",
    "\n",
    "        residual = x\n",
    "        residual = self.conv4(residual)\n",
    "        residual = self.bn4(residual)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block):\n",
    "        self.inplanes = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer1 = BasicBlock(3,64, 2)\n",
    "        self.layer2 = BasicBlock(64,128, 2)\n",
    "        self.layer3 = BasicBlock(128,256, 2)\n",
    "        self.layer4 = BasicBlock(256,512, 1)\n",
    "#         UNCOMMENT ONLY FOR COVARIANCE POOLING\n",
    "#         self.layer5 = nn.Conv2d(512, 128, kernel_size=1, stride=1)\n",
    "\n",
    "\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # TODO: is this fine?\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "#         UNCOMMENT ONLY FOR COVARIANCE POOLING\n",
    "#         x = self.layer5(x)\n",
    "        return x/math.sqrt(network_width)\n",
    "\n",
    "\n",
    "def resnet12():\n",
    "    \"\"\"Constructs a ResNet-12 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Make Models\n",
    "\n",
    "\n",
    "models = [resnet12().cuda() for i in range(ensemble)]\n",
    "\n",
    "# Load saved parameters\n",
    "model_state = torch.load(model)\n",
    "\n",
    "for i in range(ensemble):\n",
    "    models[i].load_state_dict(model_state[i])\n",
    "    models[i].eval()\n",
    "    # Zero out the bias on the final layer, since it doesn't do anything\n",
    "#     models[i].encode.process[-1].layers[-1].bias.data.zero_()\n",
    "\n",
    "# Load additional parameters for parametric localizer models\n",
    "# if localizing and not fewshot_local:\n",
    "#     fbcentroids = torch.load(model[:model.rfind('.')]+'_localizers'+model[model.rfind('.'):])\n",
    "#     for i in range(ensemble):\n",
    "#         expanders[i].centroids.data = fbcentroids[i]\n",
    "#         expanders[i].cuda()\n",
    "\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                    EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodic_eval(test_loader, models, way, shots, verbosity):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    nqueries = shots[-1]\n",
    "    ensemble = len(models)\n",
    "    targ = torch.LongTensor([i//nqueries for i in range(nqueries*way)]).cuda()\n",
    "    print(targ.shape)\n",
    "    acctracker = [[] for i in range(ensemble)]\n",
    "    print(\"Training images covered this round:\")\n",
    "    for i, ((inp, masks), _) in enumerate(test_loader):\n",
    "        inp = inp.cuda()\n",
    "        masks = masks.cuda()\n",
    "        for j in range(ensemble):\n",
    "            # Predict, step\n",
    "            out = models[j](inp)\n",
    "#             if i == 0:\n",
    "#                 print(out.shape)\n",
    "#                 print(out)\n",
    "            _,bins = torch.max(out,1)\n",
    "            print(bins.shape)\n",
    "            acc = torch.sum(torch.eq(bins,targ)).item()/nqueries/way\n",
    "            acctracker[j].append(acc)\n",
    "        if i%verbosity == 0:\n",
    "            print('%d of approx. 192270'%(i*way*sum(shots)))\n",
    "    all_acc = []\n",
    "    for i in acctracker:\n",
    "        for j in i:\n",
    "            all_acc.append(j)\n",
    "    mean_acc = sum(all_acc)/len(all_acc)\n",
    "    confs = 1.96*np.sqrt(np.var(all_acc)/len(all_acc))\n",
    "    return mean_acc, confs\n",
    "#     return [L/(i+1) for L in acctracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75])\n",
      "Training images covered this round:\n",
      "torch.Size([100, 10, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (75) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6519ee0a17bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisodic_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_acc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# accs = sum(acclist)/n_trials/ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# confs = 1.96*np.sqrt(np.var(acclist)/n_trials/ensemble)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b54ec0ee8b46>\u001b[0m in \u001b[0;36mepisodic_eval\u001b[0;34m(test_loader, models, way, shots, verbosity)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnqueries\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0macctracker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mverbosity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (75) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "mean_acc, confs = episodic_eval(test_loader, models, way, shots, verbosity)\n",
    "print(mean_acc*100)\n",
    "print(confs*100)\n",
    "# accs = sum(acclist)/n_trials/ensemble\n",
    "# confs = 1.96*np.sqrt(np.var(acclist)/n_trials/ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(acclist)\n",
    "# print(accs)\n",
    "# print(confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acclist = []\n",
    "# pcacclist = []\n",
    "# alldispacc = np.zeros(way)\n",
    "# for r in range(n_trials):\n",
    "#     # Accumulate foreground/background prototypes, if using\n",
    "#     fbcentroids = (accumulateFB(models, repr_loader, way, network_width, ngiv, bsize)\n",
    "#                    if include_masks else \n",
    "#                    [None]*ensemble)\n",
    "#     # Accumulate category prototypes\n",
    "#     centroids, counts = accumulate(models, repr_loader, expanders, \n",
    "#                                    fbcentroids, way, d)\n",
    "#     # Score the models\n",
    "#     allacc, dispacc, perclassacc = score(k, centroids, fbcentroids, models, \n",
    "#                                          query_loader, expanders, way)\n",
    "#     # Record statistics\n",
    "#     acclist = acclist+allacc\n",
    "#     pcacclist = pcacclist+list(perclassacc)\n",
    "#     alldispacc += dispacc\n",
    "\n",
    "# # Aggregate collected statistics\n",
    "# accs = sum(acclist)/n_trials/ensemble\n",
    "# pcaccs = sum(pcacclist)/n_trials/ensemble\n",
    "# alldispacc = alldispacc/n_trials\n",
    "# confs = 1.96*np.sqrt(np.var(acclist)/n_trials/ensemble)\n",
    "# pcconfs = 1.96*np.sqrt(np.var(pcacclist)/n_trials/ensemble)\n",
    "\n",
    "# # Report\n",
    "# print(\"Accuracies and 95% confidence intervals\")\n",
    "# print(\"Mean accuracy: \\t\\t%.2f \\t+/- %.2f\" % (accs*100, confs*100))\n",
    "# print(\"Per-class accuracy: \\t%.f \\t+/- %.2f\" % (pcaccs*100, pcconfs*100))\n",
    "# logcounts = [np.log10(c) for c in counts]\n",
    "# pl.figure()\n",
    "# pl.axhline(0,color='k')\n",
    "# pl.scatter(counts, dispacc*100, s=4)\n",
    "# z = np.polyfit(logcounts, np.array(dispacc)*100, 1)\n",
    "# p = np.poly1d(z)\n",
    "# pl.plot([min(counts),max(counts)], [p(min(logcounts)),p(max(logcounts))], \"r--\")\n",
    "# pl.ylim([0,100])\n",
    "# pl.xlabel('# Reference Images')\n",
    "# pl.ylabel('Percentage Points')\n",
    "# pl.xscale('log')\n",
    "# pl.title('Per-Class Top-%d Accuracy' % k)\n",
    "# pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fewshot",
   "language": "python",
   "name": "fewshot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
