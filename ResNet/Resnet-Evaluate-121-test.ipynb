{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/at677/fewshotlocal/ResNet'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import NLLLoss\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from helpful_files.networks import PROTO, avgpool, covapool, pL, pCL, fsL, fsCL, fbpredict\n",
    "from helpful_files.testing2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Important Values\n",
    "\n",
    "# General settings\n",
    "datapath = '/data/dww78/mini_inat_shrunk/'   # The location of your train, test, repr, and query folders. Make sure it ends in '/'!\n",
    "model = 'ResNet-121.pth'  # What model do you wish to evaluate, and where is it saved?\n",
    "gpu = 2                             # What gpu do you wish to run on?\n",
    "workers = 1                         # Number of cpu worker processes to use for data loading\n",
    "verbosity = 10                      # How many categories in between status updates \n",
    "ensemble = 4                        # How many models to evaluate in parallel\n",
    "k = 1                               # Evaluate top-k accuracy. Typically 1 or 5. \n",
    "torch.cuda.set_device(gpu) \n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Model characteristics\n",
    "covariance_pooling = True           # Did your model use covariance pooling?\n",
    "localizing = True                   # Did your model use localization?\n",
    "fewshot_local = False               # If you used localization: few-shot, or parametric? Few-shot if True, param if False\n",
    "network_width = 128                  # Number of channels at every layer of the network\n",
    "\n",
    "# Batch construction\n",
    "bsize = 64                          # Batch size\n",
    "boxes_available = 10                # Percentage of images with bounding boxes available (few-shot localization models only)\n",
    "include_masks = (localizing         # Include or ignore the bounding box annotations?\n",
    "                 and fewshot_local)\n",
    "n_trials = (10                      # Number of trials (few-shot localization models only)\n",
    "            if include_masks else 1)\n",
    "\n",
    "\n",
    "# Calculate embedding size based on model setup\n",
    "d = (network_width if not \n",
    "     covariance_pooling else\n",
    "     network_width**2)\n",
    "if localizing and not covariance_pooling:\n",
    "    d = network_width*2\n",
    "assert n_trials == 1 or include_masks, (\"Repeated trials will yield repeated identical results under this configuration.\"+\n",
    "                                        \"Please set ntrials to 1 or use a few-shot localizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load Testing Data\n",
    "\n",
    "d_boxes = torch.load('/data/db638/github/fewshotlocal/helpful_files/box_coords.pth')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4905, 0.4961, 0.4330],std=[0.1737, 0.1713, 0.1779])\n",
    "    ])\n",
    "\n",
    "repr_dataset = datasets.ImageFolder(\n",
    "    datapath+'repr', \n",
    "    loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "query_dataset = datasets.ImageFolder(\n",
    "    datapath+'query',\n",
    "    loader = lambda x: load_transform(x, d_boxes, transform, include_masks))\n",
    "repr_loader = torch.utils.data.DataLoader(\n",
    "    repr_dataset, \n",
    "    batch_sampler = OrderedSampler(repr_dataset, bsize),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "query_loader = torch.utils.data.DataLoader(\n",
    "    query_dataset,\n",
    "    batch_sampler = OrderedSampler(query_dataset, bsize),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "way = len(repr_dataset.classes)\n",
    "\n",
    "# Determine number of images with bounding boxes per-class\n",
    "catsizes = torch.LongTensor(np.array([t[1] for t in repr_dataset.imgs])).bincount().float()\n",
    "ngiv = (catsizes*boxes_available//100)\n",
    "for i in range(ngiv.size(0)):\n",
    "    if ngiv[i] == 0:\n",
    "        ngiv[i] = 1\n",
    "ngiv = ngiv.long().tolist()\n",
    "\n",
    "print('Data loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d_boxes = dict()\n",
    "for key, value in d_boxes.items():\n",
    "    new_key = \"/data/dww78/mini_inat_shrunk/\"+key[3:]\n",
    "    new_d_boxes[new_key] = value\n",
    "\n",
    "d_boxes = new_d_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/dww78/mini_inat_shrunk/train/3171/4c72b1a7b6a86b8de95425db8cd03384.bmp\n"
     ]
    }
   ],
   "source": [
    "print(list(d_boxes.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv3 = conv3x3(planes, planes)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "      \n",
    "        self.conv4 = conv3x3(inplanes, planes)\n",
    "        self.bn4 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(stride)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.num_batches_tracked = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.num_batches_tracked += 1\n",
    "\n",
    "        residual = x\n",
    "        residual = self.conv4(residual)\n",
    "        residual = self.bn4(residual)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block):\n",
    "        self.inplanes = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer1 = BasicBlock(3,64, 2)\n",
    "        self.layer2 = BasicBlock(64,128, 2)\n",
    "        self.layer3 = BasicBlock(128,256, 2)\n",
    "        self.layer4 = BasicBlock(256,512, 1)\n",
    "        self.layer5 = nn.Conv2d(512, 128, kernel_size=1, stride=1)\n",
    "        \n",
    "\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # TODO: is this fine?\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        return x/math.sqrt(network_width)\n",
    "\n",
    "\n",
    "def resnet12():\n",
    "    \"\"\"Constructs a ResNet-12 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Make Models\n",
    "    \n",
    "# models = [PROTO(network_width).cuda() for i in range(ensemble)]\n",
    "models = [resnet12().cuda() for i in range(ensemble)]\n",
    "\n",
    "expander = avgpool()\n",
    "if localizing:\n",
    "    if fewshot_local:\n",
    "        expander = fsCL if covariance_pooling else fsL\n",
    "    else:\n",
    "        expander = pCL(network_width) if covariance_pooling else pL(network_width)\n",
    "elif covariance_pooling:\n",
    "    expander = covapool\n",
    "expanders = [expander for _ in range(ensemble)]\n",
    "\n",
    "# Load saved parameters\n",
    "model_state = torch.load(model)\n",
    "for i in range(ensemble):\n",
    "    models[i].load_state_dict(model_state[i])\n",
    "    models[i].eval()\n",
    "    # Zero out the bias on the final layer, since it doesn't do anything\n",
    "    # models[i].process[-1].layers[-1].bias.data.zero_()\n",
    "\n",
    "# Load additional parameters for parametric localizer models\n",
    "if localizing and not fewshot_local:\n",
    "    fbcentroids = torch.load(model[:model.rfind('.')]+'_localizers'+model[model.rfind('.'):])\n",
    "    for i in range(ensemble):\n",
    "        expanders[i].centroids.data = fbcentroids[i]\n",
    "        expanders[i].cuda()\n",
    "\n",
    "print(\"Ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                    EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6272c72f1b69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpcaccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpcacclist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mpcconfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.96\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpcacclist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Per-class accuracy: \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mperclassacc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpcconfs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mlogcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "acclist = []\n",
    "pcacclist = []\n",
    "alldispacc = np.zeros(way)\n",
    "\n",
    "\n",
    "for r in range(n_trials):\n",
    "    # Accumulate foreground/background prototypes, if using\n",
    "    fbcentroids = (accumulateFB(models, repr_loader, way, network_width, ngiv, bsize)\n",
    "                   if include_masks else \n",
    "                   [None]*ensemble)\n",
    "    # Accumulate category prototypes\n",
    "    centroids, counts = accumulate(models, repr_loader, expanders, \n",
    "                                   fbcentroids, way, d)\n",
    "    # Score the models\n",
    "    allacc, perclassacc = score(k, centroids, fbcentroids, models, \n",
    "                                         query_loader, expanders, way)\n",
    "\n",
    "\n",
    "    # Record statistics\n",
    "#     acclist = acclist+allacc\n",
    "    pcacclist = pcacclist+list(perclassacc)\n",
    "#     alldispacc += dispacc\n",
    "\n",
    "# Aggregate collected statistics\n",
    "# accs = sum(acclist)/n_trials/ensemble\n",
    "\n",
    "# alldispacc = alldispacc/n_trials\n",
    "# confs = 1.96*np.sqrt(np.var(acclist)/n_trials/ensemble)\n",
    "# pcaccs = sum(pcacclist[i])/n_trials/ensemble\n",
    "# pcconfs = 1.96*np.sqrt(np.var(pcacclist[i])/n_trials/ensemble)\n",
    "\n",
    "# Report\n",
    "# print(\"Accuracies and 95% confidence intervals\")\n",
    "# print(\"Mean accuracy: \\t\\t%.2f \\t+/- %.2f\" % (accs*100, confs*100))\n",
    "# print(pcacclist.shape)\n",
    "\n",
    "for i in range(ensemble):\n",
    "    pcaccs = sum(pcacclist[i])/n_trials\n",
    "    pcconfs = 1.96*np.sqrt(np.var(pcacclist[i]))\n",
    "    print(\"Per-class accuracy: \", (perclassacc[i], pcconfs*100))\n",
    "    logcounts = [np.log10(c) for c in counts]\n",
    "    pl.figure()\n",
    "    pl.axhline(0,color='k')\n",
    "    pl.scatter(counts, perclassacc[i]*100, s=4)\n",
    "    z = np.polyfit(logcounts, np.array(perclassacc[i])*100, 1)\n",
    "    p = np.poly1d(z)\n",
    "    pl.plot([min(counts),max(counts)], [p(min(logcounts)),p(max(logcounts))], \"r--\")\n",
    "    pl.ylim([0,100])\n",
    "    pl.xlabel('# Reference Images')\n",
    "    pl.ylabel('Percentage Points')\n",
    "    pl.xscale('log')\n",
    "    pl.title('Per-Class Top-%d Accuracy' % k)\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!file /data/db638/repr/1027/bafc96ec591660f9dd262d54d8ec1d48.bmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fewshot",
   "language": "python",
   "name": "fewshot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
